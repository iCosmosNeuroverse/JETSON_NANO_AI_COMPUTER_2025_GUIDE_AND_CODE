# JETSON_NANO_AI_COMPUTER_2025_GUIDE & CODE FOR SELF DRIVING

Side note: This nano unit I cherish. I have had this Jetson nano unit for about 7 years now. It's the same unit that ended up getting me featured by Nvidia for pothole detection project that I had posted. NVIDIA still remains world's most powerful AI driver a company worth over 3 trillion. Go to [Nvidia Jetson feature page](https://developer.nvidia.com/embedded/community/jetson-projects) and search for "pothole" then select smart pothole detector project that pops up in the results, or check out the [short link](https://developer.nvidia.com/embedded/community/jetson-projects/ai_pothole_detector).


# Guide & Code to get Jetson Nano B01 4GB up and running Self Driving using Android Phone as camera (incase your CSI camera port is damaged), or via the same CSI Camera if on board camera is okay. with Jetson nano Ai computer unit as the brain

Use command below to see how much ram your system has. If like mine, your nano has 4 usb ports, 1 CSI camera port, a power jack and micro usb power jack, you have the B01 4GB nano ai computer kit with 4gb. If you have 2 camera ports, you have B02.

```
cat /proc/meminfo | head
```

_Side note: In case you bricked your jetson nano somehow via software, however particularly without physical damage, [follow my little guide to unbrick it](https://www.instagram.com/p/DSGw4dyDi2q/)._


# ON A WINDOWS HOST OR LINUX HOST MACHINE:

1. Download FEI's donkey car/jetcar SD card [prebuilt image](https://peter115342.github.io/FEI_jetracer/docs/FEIcar/FEIcar_installation/) with all dependencies. This repo doesn't immediately provide self driving, as it showcases an object detection demo driven by a human controlling a jetson nano enabled rc car. However, the environment is crucially well established to facilitate self driving, the same I will eventually apply to my full scale hypercar prototype.
2. Remove sd card from nano.
3. Use adaptor, plug in host computer, then format sd card with sd card formatter.4. Burn image with Balena Echer.


# ON NANO:


4. [Install swap file](https://jkjung-avt.github.io/setting-up-nano/) to nano to avoid memory issues. An example of memory issue is your on board CSI camera working once, but failing afterwords. Note 8gb file in /mnt/. Also note your total free space by observing Ubuntu's "Other locations" folder and "Computer", which are virtual locations to look on data about your root folder size.

Confirm swap with 

```
swapon --show
```

or

```
free -h 
```

If you don't make the swap permanent as the tutorial above shows you only need to run swapon after reboot.

```
sudo swapon /mnt/8gb.swap
```

Other helpful commands

```
sudo nano /etc/fstab  (before entering /etc/fstab to add entry
...enter file and add entry as seen in tutorial...

sudo chmod 600 /mnt/8gb.swap --secure swap after mk command


sudo swapoff /mnt/8gb.swap  --turn off swap
sudo swapon -a  --turn on swap
swapon --show  --verify on
```

So a straightforward route looks like:

```
mk command to make swap file > chmod to secure swap file > make it permanent if desired (or just run swapon command with the file name upon reboot) > show command to verify swap
```


6. Time to run some demos. Open terminal and run commands:

```
>>> source env/bin/activate
>>> cd ~/mycar
```

# 6. Copy my python files self_drive_basic.py, self_drive.py, and csi_cam_test.py, and  ip_cam_test.py to the "mycar" directory.

Note, before invoking drive on the nano, if you just have a camera and no servo /steering/throttle pwm yet attached to NANO, you can do a dev test by disabling the following /home/mycar/myconfig.py. Add the following lines to the top of the file after "YOLO_NMS_THRESHOLD = 0.4" and before the commented out section. Ensure they aren't commented out:

```
DRIVE_TRAIN_TYPE="mock"
CONTROLLER_TYPE="mock"
USE_JOYSTICK_AS_DEFAULT=False
HAVE_IMU = False
USE_SSD1306_128_32 = False
HAVE_ODOM = False
USE_LIDAR = False
HAVE_TFMINI = False
HAVE_RGB_LED = False
```

Before next command, TEST YOUR NANO CAMERA FIRST, a window should pop up showing feed based on your onboard jetson camera:

```
>>> gst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM), width=1280, height=720, framerate=30/1' ! nvvidconv ! 'video/x-raw, format=BGRx' ! videoconvert ! xvimagesink
```

You can talso test ip cam by invoking my csi_cam_test.py file. >>> python csi_cam_test.py

7. Time to invoke object detection using jetson nano:

```
>>> python manage_.py drive 
```

8. FOR USERS WITHOUT CSI CAMERA:

Invoke my modified manage_.py file if you don't have nano CSI camera, and you have android and IPWebcam app. Start app, then start server on phone which shows a camera stream.

You can test ip cam by invoking my ip_cam_test.py file. >>> python ip_cam_test.py

```
>>> python manage.py drive --ip_cam_url="<Your phone ip cam url or other device ip cam url>"
```

Eg:

```
>>> python manage_.py drive --ip_cam_url="http://192.168.100.192:8080/video" (manage_.py was originally from Fei, but I modified and created a separate file "manage_" to have IPCamera usage functionality)
```

[Acquire donkey pretrained models](https://github.com/autorope/donkey_datasets) from circuit_launch_20210716/models and place them in mycar/models folder. Integrate when asked. 

Use pretrained pilot ai for self driving inference with external ipcamera via phone from playstore like IPWebcam on android  or onboard CSICamera

#Example invoke

```
>>> python self_drive.py "http://192.168.100.192:8080/video" "models/pilot_21-08-12_4.h5"
```

#Example invoke

```
>>> python self_drive.py "models/pilot_21-08-12_4.h5" (uses csi onboard instead of external ip android cam)
```

## Alternatively, where Fei_Jetracer utilizes default human in the loop based wb control ui via separate host machine connection to running nano unit, via >>> python manage.py drive, which is standard DonkeyCar behaviour, we can also invoke autonomous driving without writing another python file as I did above, by following:

```
>>> python manage.py drive --model <your model path/model name.extension>
```

Example:

```
>>> python manage.py drive --model models/pilot.h5
```

Use non ml for self driving inference with external ipcamera via phone from playstore like IPWebcam on android  or onboard CSICamera

#Example invoke

```
>>> python self_drive_basic.py "http://192.168.100.192:8080/video" "models/pilot_21-08-12_4.h5" (External IP camera android phone etc feed)
```

#Example invoke

```
>>> python self_drive_basic.py "models/pilot_21-08-12_4.h5" (uses csi onboard instead of external ip android cam)
```

Note your nano ip. You can navigate to it on host  computer and drive the nano by ui controls if you had joystick. May be possible to use a separate mobile. Eg my nano ip:

```
http://192.168.100.197:8887
```


# An example road clip you can use to simulate jetson travelling , by focusing camera on a computer screen playing the clip. You can use nano itself to run the video while focusing it's own 
camera on the clip though that could be costly memory wise.

This way you'd have what I call "augmented and partially realistic self driving" test where nano is seeing the road and responding by sending out it's output signals though not to PWM modules to servos and wheels.

Could be useful before attaching wheels and steering to nano.

[https://youtu.be/Nhg4BjgkWGQ](https://youtu.be/Nhg4BjgkWGQ)




